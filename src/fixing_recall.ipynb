{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "34d8c425-dcdb-4e34-a38e-45b742f45a82",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch.optim as optim\n",
    "\n",
    "from utils.nets import *\n",
    "from utils.model_tools import *\n",
    "from utils.feature_extractor import *\n",
    "from utils.dataset_tools import *\n",
    "from utils.cosine_similarity import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1ab15007-a1f9-4b32-9e8b-ee89098d15f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz to ./data/FashionMNIST/raw/train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "35659fc9295446d587c02e5a70e08c95",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/26421880 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/FashionMNIST/raw/train-images-idx3-ubyte.gz to ./data/FashionMNIST/raw\n",
      "\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz to ./data/FashionMNIST/raw/train-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "25d34f0cfd5b4be58a9951f729b74127",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/29515 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/FashionMNIST/raw/train-labels-idx1-ubyte.gz to ./data/FashionMNIST/raw\n",
      "\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz to ./data/FashionMNIST/raw/t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9cffac22285148d8a0d7f7126125bf7a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4422102 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/FashionMNIST/raw/t10k-images-idx3-ubyte.gz to ./data/FashionMNIST/raw\n",
      "\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz to ./data/FashionMNIST/raw/t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e6bf27a501e04e45b67a9f767e2243a6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5148 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/FashionMNIST/raw/t10k-labels-idx1-ubyte.gz to ./data/FashionMNIST/raw\n",
      "\n"
     ]
    }
   ],
   "source": [
    "transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.5), (0.5))])\n",
    "\n",
    "LEARNING_RATE = 0.001\n",
    "EXP_DECAY = 0.0001\n",
    "\n",
    "batch_size = 64\n",
    "\n",
    "FMNIST_train_gen = torchvision.datasets.FashionMNIST(root='./data', train=True,\n",
    "                                        download=True, transform=transform)\n",
    "FMNIST_trainloader_gen = torch.utils.data.DataLoader(FMNIST_train_gen, batch_size=batch_size,\n",
    "                                          shuffle=True, num_workers=2)\n",
    "\n",
    "FMNIST_test_gen = torchvision.datasets.FashionMNIST(root='./data', train=False,\n",
    "                                       download=True, transform=transform)\n",
    "FMNIST_testloader_gen = torch.utils.data.DataLoader(FMNIST_test_gen, batch_size=batch_size,\n",
    "                                         shuffle=False, num_workers=2)\n",
    "\n",
    "\n",
    "no_boot_bag_train_idx = np.where((np.array(FMNIST_train_gen.targets) != 8) & \n",
    "                        (np.array(FMNIST_train_gen.targets) != 9))[0]\n",
    "no_boot_bag_train_subset = torch.utils.data.Subset(FMNIST_train_gen, no_boot_bag_train_idx)\n",
    "no_boot_bag_train_dl = torch.utils.data.DataLoader(no_boot_bag_train_subset, batch_size=batch_size, shuffle=True, num_workers=2)\n",
    "\n",
    "no_boot_bag_test_idx = np.where((np.array(FMNIST_test_gen.targets) != 8) & \n",
    "                        (np.array(FMNIST_test_gen.targets) != 9))[0]\n",
    "no_boot_bag_test_subset = torch.utils.data.Subset(FMNIST_test_gen, no_boot_bag_test_idx)\n",
    "no_boot_bag_test_dl = torch.utils.data.DataLoader(no_boot_bag_test_subset, batch_size=batch_size, shuffle=True, num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6f938929-4176-4db5-b476-5aec535e104d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearFashionMNIST_alt(nn.Module):\n",
    "  def __init__(self, input_size, num_classes: int):\n",
    "    super(LinearFashionMNIST_alt, self).__init__()\n",
    "\n",
    "    self.flatten = nn.Flatten()\n",
    "    self.input_layer = nn.Linear(input_size, 128)\n",
    "    self.output_layer = nn.Linear(128, num_classes)\n",
    "\n",
    "  def forward(self, x):\n",
    "    x = self.flatten(x)\n",
    "    return self.output_layer(self.input_layer(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "08823b85-c172-4e88-8c72-839203438829",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "linear_model = LinearFashionMNIST_alt(28*28, 8)\n",
    "FMNIST_optim = optim.Adam(linear_model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "num_epochs = 10\n",
    "\n",
    "decay_rate = (EXP_DECAY/LEARNING_RATE)**(1/num_epochs)\n",
    "\n",
    "lr_scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer=FMNIST_optim, gamma=decay_rate) \n",
    "# TODO: we need to use the scheduler for cnn too if we use that"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3fbcdfab-50df-4a5c-9b96-c23689804ecd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.exceptions import ArchitectureError\n",
    "\n",
    "import torchmetrics\n",
    "from torchmetrics.classification import MulticlassRecall\n",
    "\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "165ce0cb-b98c-4aba-bd19-c2ffecf18df3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(dataloader, model, loss_fn, device, swap=False, swap_labels=[], classes = 9) -> float:\n",
    "    '''\n",
    "        Model test loop. Performs a single epoch of model updates.\n",
    "\n",
    "        * USAGE *\n",
    "        Within a training loop of range(num_epochs) to perform epoch validation, or after training to perform testing.\n",
    "\n",
    "        * PARAMETERS *\n",
    "        dataloader: A torch.utils.data.DataLoader object\n",
    "        model: A torch model which subclasses torch.nn.Module\n",
    "        loss_fn: A torch loss function, such as torch.nn.CrossEntropyLoss\n",
    "        optimizer: A torch.optim optimizer\n",
    "        device: 'cuda' or 'cpu'\n",
    "\n",
    "        * RETURNS *\n",
    "        float: The average test loss\n",
    "    '''\n",
    "\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    test_loss, correct = 0, 0\n",
    "    y_pred_list, targets = [], []\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            if swap:\n",
    "                for i in range(len(y)):\n",
    "                    if y[i] == swap_labels[0]:\n",
    "                        y[i] = swap_labels[1]\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            pred = model(X)\n",
    "            #preds.append(pred)\n",
    "            targets.append(y.numpy())\n",
    "            test_loss += loss_fn(pred, y).item()\n",
    "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "            \n",
    "            _, y_pred_tags = torch.max(pred, dim=1)\n",
    "            y_pred_list.append(y_pred_tags.cpu().numpy())\n",
    "            \n",
    "    y_pred_list = [a.squeeze().tolist() for a in y_pred_list]\n",
    "\n",
    "    test_loss /= num_batches\n",
    "    correct /= size\n",
    "    \n",
    "    #print(preds)\n",
    "    #print(targets)\n",
    "    \n",
    "    recall = MulticlassRecall(classes)\n",
    "    # torch.IntTensor(targets)\n",
    "    recall_val = recall(torch.FloatTensor(np.asarray(y_pred_list)), torch.IntTensor(np.asarray(targets)))\n",
    "    # should I be calling it on preds[0]?\n",
    "\n",
    "    print(\n",
    "        f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f}, Recall val: {recall_val:>8f} \\n\")\n",
    "\n",
    "    return test_loss, np.asarray(y_pred_list), np.asarray(targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "92b6a694-747c-412e-9106-1d2ed9d8a60d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 2.073817  [    0/48000]\n",
      "Test Error: \n",
      " Accuracy: 81.0%, Avg loss: 0.516074, Recall val: 0.720444 \n",
      "\n",
      "Epoch 0 train loss: 0.5466629227797191 test loss: 0.5160740908384324\n",
      "loss: 0.451285  [    0/48000]\n",
      "Test Error: \n",
      " Accuracy: 81.1%, Avg loss: 0.513701, Recall val: 0.721222 \n",
      "\n",
      "Epoch 1 train loss: 0.4661607717871666 test loss: 0.5137012100219727\n",
      "loss: 0.508286  [    0/48000]\n",
      "Test Error: \n",
      " Accuracy: 81.9%, Avg loss: 0.494878, Recall val: 0.727778 \n",
      "\n",
      "Epoch 2 train loss: 0.4480654340386391 test loss: 0.4948781633377075\n",
      "loss: 0.356095  [    0/48000]\n",
      "Test Error: \n",
      " Accuracy: 82.5%, Avg loss: 0.479291, Recall val: 0.733778 \n",
      "\n",
      "Epoch 3 train loss: 0.43992770957946775 test loss: 0.4792911456823349\n",
      "loss: 0.334604  [    0/48000]\n",
      "Test Error: \n",
      " Accuracy: 82.3%, Avg loss: 0.489301, Recall val: 0.731333 \n",
      "\n",
      "Epoch 4 train loss: 0.4306096281806628 test loss: 0.4893008120059967\n",
      "loss: 0.320243  [    0/48000]\n",
      "Test Error: \n",
      " Accuracy: 83.0%, Avg loss: 0.473497, Recall val: 0.737556 \n",
      "\n",
      "Epoch 5 train loss: 0.42250396474202473 test loss: 0.4734969985485077\n",
      "loss: 0.339288  [    0/48000]\n",
      "Test Error: \n",
      " Accuracy: 82.8%, Avg loss: 0.469426, Recall val: 0.736222 \n",
      "\n",
      "Epoch 6 train loss: 0.41803533653418223 test loss: 0.4694258699417114\n",
      "loss: 0.406882  [    0/48000]\n",
      "Test Error: \n",
      " Accuracy: 83.0%, Avg loss: 0.464947, Recall val: 0.737333 \n",
      "\n",
      "Epoch 7 train loss: 0.4140849905014038 test loss: 0.46494732761383056\n",
      "loss: 0.572361  [    0/48000]\n",
      "Test Error: \n",
      " Accuracy: 82.9%, Avg loss: 0.463489, Recall val: 0.736889 \n",
      "\n",
      "Epoch 8 train loss: 0.4097398666739464 test loss: 0.46348937356472014\n",
      "loss: 0.561619  [    0/48000]\n",
      "Test Error: \n",
      " Accuracy: 83.2%, Avg loss: 0.464321, Recall val: 0.739556 \n",
      "\n",
      "Epoch 9 train loss: 0.40770433831214903 test loss: 0.46432097125053406\n"
     ]
    }
   ],
   "source": [
    "train_losses = []\n",
    "test_losses = []\n",
    "y_preds = []\n",
    "y_tests = []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    train_loss = train(no_boot_bag_train_dl, linear_model, criterion, FMNIST_optim, 'cpu')\n",
    "    test_loss, y_pred_list, y_test = test(no_boot_bag_test_dl, linear_model, criterion, 'cpu')\n",
    "    y_preds.append(y_pred_list)\n",
    "    y_tests.append(y_test)\n",
    "    \n",
    "    print(\"Epoch\", epoch, \"train loss:\", train_loss, \"test loss:\", test_loss)\n",
    "    \n",
    "    lr_scheduler.step()\n",
    "    \n",
    "    train_losses.append(train_loss)\n",
    "    test_losses.append(test_loss)\n",
    "    \n",
    "    #print(classification_report(y_test, y_pred_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "329ab204-cf4b-43c9-9bff-78959c9be84b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.2771250009536743, 0.2761249840259552, 0.27250000834465027, 0.28700000047683716, 0.2709999978542328, 0.2731249928474426, 0.28712499141693115, 0.2861250042915344, 0.28224998712539673, 0.2789999842643738]\n"
     ]
    }
   ],
   "source": [
    "target_classes = [1,2,6]\n",
    "num_classes = 8\n",
    "# I thought you would use len(target_classes), and instead I get this complaint: Detected more unique \n",
    "# values in `preds` than `num_classes`. Expected only 3 but found 6 in `preds`.\n",
    "\n",
    "recall_per_epoch = []\n",
    "\n",
    "for e in range(num_epochs):\n",
    "    recall = MulticlassRecall(num_classes)\n",
    "    \n",
    "    y_per_epoch = np.asarray(y_tests[e]).flatten()\n",
    "    preds_per_epoch = np.asarray(y_preds[e]).flatten()\n",
    "    \n",
    "    condition = y_per_epoch == target_classes[0]\n",
    "    for i in range(1, len(target_classes)):\n",
    "        condition |= y_per_epoch == target_classes[i]\n",
    "    \n",
    "    target_y = np.extract(condition, y_per_epoch)\n",
    "    target_preds = np.extract(condition, preds_per_epoch)\n",
    "    \n",
    "    recall_val = recall(torch.IntTensor(target_preds), torch.IntTensor(target_y))\n",
    "    \n",
    "    recall_per_epoch.append(recall_val.item())\n",
    "        \n",
    "print(recall_per_epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "9287f797-7c9d-4d7f-a0a8-32153a7dfde7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[], [], []]\n"
     ]
    }
   ],
   "source": [
    "target_classes = [1,2,3]\n",
    "recall_per_class = [[]] * len(target_classes)\n",
    "print(recall_per_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "15ec3ab1-e287-468d-b127-ea9c769cc0d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'int'>\n",
      "0.823737519979477\n"
     ]
    }
   ],
   "source": [
    "#target_classes = [1,2]\n",
    "nc = 8\n",
    "\n",
    "print(type(num_classes))\n",
    "\n",
    "recall_per_epoch = []\n",
    "recall = MulticlassRecall(nc)\n",
    "\n",
    "for i in range(num_epochs):\n",
    "    y_epoch = np.asarray(y_tests[i]).flatten()\n",
    "    yhat_epoch = np.asarray(y_preds[i]).flatten()\n",
    "    \n",
    "    y_epoch_ = []\n",
    "    yhat_epoch_ = []\n",
    "    \n",
    "    for j in range(nc):\n",
    "        yf = np.where(y_epoch == j)[0]\n",
    "        tempy = y_epoch[yf]\n",
    "        tempyhat = yhat_epoch[yf]\n",
    "        \n",
    "        y_epoch_.append(tempy)\n",
    "        yhat_epoch_.append(tempyhat)\n",
    "        \n",
    "    recall_val = recall(torch.IntTensor(yhat_epoch_),torch.IntTensor(y_epoch_))\n",
    "    recall_per_epoch.append(recall_val.item())\n",
    "    \n",
    "print(np.mean(recall_per_epoch))\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "0ed366f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.max(y_epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43fda75d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
