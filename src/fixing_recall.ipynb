{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "34d8c425-dcdb-4e34-a38e-45b742f45a82",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch.optim as optim\n",
    "\n",
    "from utils.nets import *\n",
    "from utils.model_tools import *\n",
    "from utils.feature_extractor import *\n",
    "from utils.dataset_tools import *\n",
    "from utils.cosine_similarity import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1ab15007-a1f9-4b32-9e8b-ee89098d15f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.5), (0.5))])\n",
    "\n",
    "LEARNING_RATE = 0.001\n",
    "EXP_DECAY = 0.0001\n",
    "\n",
    "batch_size = 64\n",
    "\n",
    "FMNIST_train_gen = torchvision.datasets.FashionMNIST(root='./data', train=True,\n",
    "                                        download=True, transform=transform)\n",
    "FMNIST_trainloader_gen = torch.utils.data.DataLoader(FMNIST_train_gen, batch_size=batch_size,\n",
    "                                          shuffle=True, num_workers=2)\n",
    "\n",
    "FMNIST_test_gen = torchvision.datasets.FashionMNIST(root='./data', train=False,\n",
    "                                       download=True, transform=transform)\n",
    "FMNIST_testloader_gen = torch.utils.data.DataLoader(FMNIST_test_gen, batch_size=batch_size,\n",
    "                                         shuffle=False, num_workers=2)\n",
    "\n",
    "\n",
    "no_boot_bag_train_idx = np.where((np.array(FMNIST_train_gen.targets) != 8) & \n",
    "                        (np.array(FMNIST_train_gen.targets) != 9))[0]\n",
    "no_boot_bag_train_subset = torch.utils.data.Subset(FMNIST_train_gen, no_boot_bag_train_idx)\n",
    "no_boot_bag_train_dl = torch.utils.data.DataLoader(no_boot_bag_train_subset, batch_size=batch_size, shuffle=True, num_workers=2)\n",
    "\n",
    "no_boot_bag_test_idx = np.where((np.array(FMNIST_test_gen.targets) != 8) & \n",
    "                        (np.array(FMNIST_test_gen.targets) != 9))[0]\n",
    "no_boot_bag_test_subset = torch.utils.data.Subset(FMNIST_test_gen, no_boot_bag_test_idx)\n",
    "no_boot_bag_test_dl = torch.utils.data.DataLoader(no_boot_bag_test_subset, batch_size=batch_size, shuffle=True, num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6f938929-4176-4db5-b476-5aec535e104d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearFashionMNIST_alt(nn.Module):\n",
    "  def __init__(self, input_size, num_classes: int):\n",
    "    super(LinearFashionMNIST_alt, self).__init__()\n",
    "\n",
    "    self.flatten = nn.Flatten()\n",
    "    self.input_layer = nn.Linear(input_size, 128)\n",
    "    self.output_layer = nn.Linear(128, num_classes)\n",
    "\n",
    "  def forward(self, x):\n",
    "    x = self.flatten(x)\n",
    "    return self.output_layer(self.input_layer(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "08823b85-c172-4e88-8c72-839203438829",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "linear_model = LinearFashionMNIST_alt(28*28, 8)\n",
    "FMNIST_optim = optim.Adam(linear_model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "num_epochs = 15\n",
    "\n",
    "decay_rate = (EXP_DECAY/LEARNING_RATE)**(1/num_epochs)\n",
    "\n",
    "lr_scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer=FMNIST_optim, gamma=decay_rate) \n",
    "# TODO: we need to use the scheduler for cnn too if we use that"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3fbcdfab-50df-4a5c-9b96-c23689804ecd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.exceptions import ArchitectureError\n",
    "\n",
    "import torchmetrics\n",
    "from torchmetrics.classification import MulticlassRecall\n",
    "\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "165ce0cb-b98c-4aba-bd19-c2ffecf18df3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(dataloader, model, loss_fn, device, swap=False, swap_labels=[], classes = 9) -> float:\n",
    "    '''\n",
    "        Model test loop. Performs a single epoch of model updates.\n",
    "\n",
    "        * USAGE *\n",
    "        Within a training loop of range(num_epochs) to perform epoch validation, or after training to perform testing.\n",
    "\n",
    "        * PARAMETERS *\n",
    "        dataloader: A torch.utils.data.DataLoader object\n",
    "        model: A torch model which subclasses torch.nn.Module\n",
    "        loss_fn: A torch loss function, such as torch.nn.CrossEntropyLoss\n",
    "        optimizer: A torch.optim optimizer\n",
    "        device: 'cuda' or 'cpu'\n",
    "\n",
    "        * RETURNS *\n",
    "        float: The average test loss\n",
    "    '''\n",
    "\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    test_loss, correct = 0, 0\n",
    "    y_pred_list, targets = [], []\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            if swap:\n",
    "                for i in range(len(y)):\n",
    "                    if y[i] == swap_labels[0]:\n",
    "                        y[i] = swap_labels[1]\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            pred = model(X)\n",
    "            #preds.append(pred)\n",
    "            targets.append(y.numpy())\n",
    "            test_loss += loss_fn(pred, y).item()\n",
    "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "            \n",
    "            _, y_pred_tags = torch.max(pred, dim=1)\n",
    "            y_pred_list.append(y_pred_tags.cpu().numpy())\n",
    "            \n",
    "    y_pred_list = [a.squeeze().tolist() for a in y_pred_list]\n",
    "\n",
    "    test_loss /= num_batches\n",
    "    correct /= size\n",
    "    \n",
    "    #print(preds)\n",
    "    #print(targets)\n",
    "    \n",
    "    recall = MulticlassRecall(classes)\n",
    "    # torch.IntTensor(targets)\n",
    "    recall_val = recall(torch.FloatTensor(np.asarray(y_pred_list)), torch.IntTensor(np.asarray(targets)))\n",
    "    # should I be calling it on preds[0]?\n",
    "\n",
    "    print(\n",
    "        f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f}, Recall val: {recall_val:>8f} \\n\")\n",
    "\n",
    "    return test_loss, np.asarray(y_pred_list), np.asarray(targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "92b6a694-747c-412e-9106-1d2ed9d8a60d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 2.038709  [    0/48000]\n",
      "Test Error: \n",
      " Accuracy: 81.5%, Avg loss: 0.501730, Recall val: 0.724333 \n",
      "\n",
      "Epoch 0 train loss: 0.5428749349514643 test loss: 0.5017296605110169\n",
      "loss: 0.386660  [    0/48000]\n",
      "Test Error: \n",
      " Accuracy: 82.3%, Avg loss: 0.487703, Recall val: 0.731667 \n",
      "\n",
      "Epoch 1 train loss: 0.47309311193227765 test loss: 0.48770318055152895\n",
      "loss: 0.397758  [    0/48000]\n",
      "Test Error: \n",
      " Accuracy: 81.2%, Avg loss: 0.497151, Recall val: 0.722222 \n",
      "\n",
      "Epoch 2 train loss: 0.4534300406376521 test loss: 0.49715096735954284\n",
      "loss: 0.653621  [    0/48000]\n",
      "Test Error: \n",
      " Accuracy: 82.1%, Avg loss: 0.481427, Recall val: 0.730000 \n",
      "\n",
      "Epoch 3 train loss: 0.44205059425036114 test loss: 0.4814268772602081\n",
      "loss: 0.281445  [    0/48000]\n",
      "Test Error: \n",
      " Accuracy: 82.5%, Avg loss: 0.484571, Recall val: 0.732889 \n",
      "\n",
      "Epoch 4 train loss: 0.43446280564864476 test loss: 0.4845711535215378\n",
      "loss: 0.343484  [    0/48000]\n",
      "Test Error: \n",
      " Accuracy: 82.0%, Avg loss: 0.483754, Recall val: 0.729000 \n",
      "\n",
      "Epoch 5 train loss: 0.4286936230858167 test loss: 0.48375377678871156\n",
      "loss: 0.503242  [    0/48000]\n",
      "Test Error: \n",
      " Accuracy: 82.5%, Avg loss: 0.473751, Recall val: 0.733556 \n",
      "\n",
      "Epoch 6 train loss: 0.42277621634801227 test loss: 0.4737505543231964\n",
      "loss: 0.334931  [    0/48000]\n",
      "Test Error: \n",
      " Accuracy: 82.2%, Avg loss: 0.477050, Recall val: 0.730556 \n",
      "\n",
      "Epoch 7 train loss: 0.41836853567759197 test loss: 0.4770500901937485\n",
      "loss: 0.375830  [    0/48000]\n",
      "Test Error: \n",
      " Accuracy: 82.9%, Avg loss: 0.466263, Recall val: 0.736556 \n",
      "\n",
      "Epoch 8 train loss: 0.4124863620003065 test loss: 0.46626322841644285\n",
      "loss: 0.460123  [    0/48000]\n",
      "Test Error: \n",
      " Accuracy: 82.6%, Avg loss: 0.469382, Recall val: 0.733889 \n",
      "\n",
      "Epoch 9 train loss: 0.41004623512427013 test loss: 0.46938220179080964\n",
      "loss: 0.244521  [    0/48000]\n",
      "Test Error: \n",
      " Accuracy: 82.6%, Avg loss: 0.478653, Recall val: 0.734000 \n",
      "\n",
      "Epoch 10 train loss: 0.40811605674028395 test loss: 0.47865297889709474\n",
      "loss: 0.600056  [    0/48000]\n",
      "Test Error: \n",
      " Accuracy: 82.8%, Avg loss: 0.463130, Recall val: 0.736000 \n",
      "\n",
      "Epoch 11 train loss: 0.4062656271656354 test loss: 0.4631304407119751\n",
      "loss: 0.330769  [    0/48000]\n",
      "Test Error: \n",
      " Accuracy: 82.9%, Avg loss: 0.463820, Recall val: 0.736556 \n",
      "\n",
      "Epoch 12 train loss: 0.4030259189804395 test loss: 0.46381975495815275\n",
      "loss: 0.290080  [    0/48000]\n",
      "Test Error: \n",
      " Accuracy: 82.7%, Avg loss: 0.462819, Recall val: 0.735444 \n",
      "\n",
      "Epoch 13 train loss: 0.40094087050358457 test loss: 0.46281910824775696\n",
      "loss: 0.266758  [    0/48000]\n",
      "Test Error: \n",
      " Accuracy: 83.1%, Avg loss: 0.464109, Recall val: 0.738444 \n",
      "\n",
      "Epoch 14 train loss: 0.39877236884832384 test loss: 0.46410880839824675\n"
     ]
    }
   ],
   "source": [
    "train_losses = []\n",
    "test_losses = []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    train_loss = train(no_boot_bag_train_dl, linear_model, criterion, FMNIST_optim, 'cpu')\n",
    "    test_loss, y_pred_list, y_test = test(no_boot_bag_test_dl, linear_model, criterion, 'cpu')\n",
    "    \n",
    "    print(\"Epoch\", epoch, \"train loss:\", train_loss, \"test loss:\", test_loss)\n",
    "    \n",
    "    lr_scheduler.step()\n",
    "    \n",
    "    train_losses.append(train_loss)\n",
    "    test_losses.append(test_loss)\n",
    "    \n",
    "    #print(classification_report(y_test, y_pred_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9287f797-7c9d-4d7f-a0a8-32153a7dfde7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
