# CS394N Term Project
CS 394N: Neural Networks @ UT Austin

The use of deep learning and neural networks has exponentially improved the ability of image classification, with neural networks being able to classify thousands of images with remarkable success. These networks often run into the issue of catastrophic forgetting, where the introduction of a new class diminishes the performance of classifying old classes. Recent work has taken inspiration from theories of learning in the brain and has shown that this issue can be reduced by reintroducing images of the old classes using Fully Interleaved Learning (FIL) or Similarly-Weighted Interleaved Learning (SWIL) while retraining on the new class. In the following work, as a continuance of this pattern of taking inspiration from the brain's learning structures, we expand on SWIL and utilize generative images as a form of noisy memory recall when retraining the network on a new class. We call this method Generative-based Similarly Weighted Interleaved Learning (G-SWIL). We first replicate the results of SWIL showing that the use of retraining on weighted old classes provides enough information to reduce the effects of catastrophic forgetting. We then retrain the network using G-SWIL which utilizes generative-based images obtained using Dall-e. Our results show that while this process has merit, the images gathered from out-of-the-box Dall-e weights are too noisy to support SWIL.
